{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10794535,"sourceType":"datasetVersion","datasetId":6699072},{"sourceId":262874,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":224809,"modelId":246553}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:59:53.831119Z","iopub.execute_input":"2025-02-19T12:59:53.831535Z","iopub.status.idle":"2025-02-19T12:59:53.850108Z","shell.execute_reply.started":"2025-02-19T12:59:53.831505Z","shell.execute_reply":"2025-02-19T12:59:53.849359Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vit_isl_model/pytorch/default/1/isl_vit_model.pth\n/kaggle/input/test-videos/IMG_8333.MOV\n/kaggle/input/test-videos/IMG_8299.MOV\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ✅ Process Video Frames without Displaying Every Frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break  # Stop when video ends\n\n    frame_count += 1\n    if frame_count % frame_skip != 0:  # Skip frames to reduce processing load\n        continue\n\n    # Convert frame to RGB (Mediapipe requirement)\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    # ✅ Detect Hands\n    results = hands.process(rgb_frame)\n\n    if results.multi_hand_landmarks:  # If hands are detected\n        hand_regions = []  # Store cropped hand images\n\n        for hand_landmarks in results.multi_hand_landmarks:\n            # Get hand bounding box\n            h, w, _ = frame.shape\n            x_min, y_min = w, h\n            x_max, y_max = 0, 0\n\n            for lm in hand_landmarks.landmark:\n                x, y = int(lm.x * w), int(lm.y * h)\n                x_min, y_min = min(x, x_min), min(y, y_min)\n                x_max, y_max = max(x, x_max), max(y, y_max)\n\n            # Expand bounding box slightly\n            padding = 20\n            x_min, y_min = max(0, x_min - padding), max(0, y_min - padding)\n            x_max, y_max = min(w, x_max + padding), min(h, y_max + padding)\n\n            # ✅ Crop Hand Region\n            hand_crop = frame[y_min:y_max, x_min:x_max]\n            if hand_crop.shape[0] == 0 or hand_crop.shape[1] == 0:\n                continue  # Skip invalid crops\n\n            hand_crop = cv2.resize(hand_crop, (112, 224))  # Resize hand to half the model input\n            hand_regions.append(hand_crop)\n\n        if len(hand_regions) == 2:  # If two hands detected, merge them\n            combined_hands = np.hstack(hand_regions)  # Stack images horizontally\n        elif len(hand_regions) == 1:  # If only one hand detected, duplicate it\n            combined_hands = np.hstack([hand_regions[0], hand_regions[0]])\n        else:\n            continue  # If no valid hands detected, skip frame\n\n        # Convert to PIL Image\n        combined_image = Image.fromarray(cv2.cvtColor(combined_hands, cv2.COLOR_BGR2RGB))\n\n        # ✅ Preprocess Image for Model\n        input_tensor = feature_extractor(combined_image).unsqueeze(0).to(device)\n\n        # ✅ Make Prediction\n        with torch.no_grad():\n            outputs = model(input_tensor)\n            predicted_class = torch.argmax(outputs.logits, dim=1).item()\n\n        # ✅ Handle Out-of-Range Predictions\n        if predicted_class >= len(class_labels):\n            predicted_letter = \"UNKNOWN\"\n        else:\n            predicted_letter = class_labels[predicted_class]\n\n        letter_queue.append(predicted_letter)\n\n        # ✅ Hold-Time Filtering: Accept the most common letter after hold_threshold frames\n        if len(letter_queue) == letter_queue.maxlen:\n            most_common_letter, count = Counter(letter_queue).most_common(1)[0]\n            if count >= hold_threshold and (not final_letters or most_common_letter != final_letters[-1]):\n                final_letters.append(most_common_letter)  # Only add if different from the last added letter\n\n        # ✅ Logging (Instead of Displaying Every Frame)\n        if frame_count % (frame_skip * 10) == 0:  # Log every 10 processed frames\n            print(f\"Frame {frame_count}: Predicted Letter - {predicted_letter}\")\n\n# ✅ Release Video\ncap.release()\n\n# ✅ Convert Filtered Letters into a Word\nfinal_word = \"\".join(final_letters)\nprint(\"\\n✅ Final Predicted Word:\", final_word)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:57:11.642937Z","iopub.execute_input":"2025-02-19T12:57:11.643349Z","iopub.status.idle":"2025-02-19T12:57:11.656156Z","shell.execute_reply.started":"2025-02-19T12:57:11.643318Z","shell.execute_reply":"2025-02-19T12:57:11.654949Z"}},"outputs":[{"name":"stdout","text":"\n✅ Final Predicted Word: HI\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import cv2\nimport torch\nimport numpy as np\nimport mediapipe as mp\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom collections import deque, Counter\nfrom torchvision import transforms\nfrom PIL import Image\nfrom google.colab.patches import cv2_imshow\n\n# ✅ Load Mediapipe Hands\nmp_hands = mp.solutions.hands\nhands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7)\n\n# ✅ Load Pretrained Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"✅ Using device: {device}\")\n\nmodel = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=35)\nmodel.load_state_dict(torch.load(\"/kaggle/input/vit_isl_model/pytorch/default/1/isl_vit_model.pth\", map_location=device))  # Load trained weights\nmodel.to(device)\nmodel.eval()\n\n# ✅ Feature Extractor (Resize + Normalize)\nfeature_extractor = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# ✅ Open Video File\nvideo_path = \"/kaggle/input/test-videos/IMG_8333.MOV\"  # Update with your actual file path\ncap = cv2.VideoCapture(video_path)\n\n# ✅ Class Labels (Modify Based on Your Dataset)\nclass_labels = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n                \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\",\n                \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n\n# ✅ Initialize Predictions Storage\nframe_skip = 5  # Process every 5th frame to reduce duplicates\nframe_count = 0\nletter_queue = deque(maxlen=10)  # Store last 10 predictions to filter noise\nhold_threshold = 5  # General threshold for letter detection\nhold_threshold_m = 1  # ✅ Adjusted threshold for \"M\" (easier to detect)\npause_threshold = 5  # ✅ Reduced pause threshold to detect sentence breaks\noverlap_threshold = 0.4  # ✅ New: Hands must overlap at least 40% to merge\nfinal_sentence = []  # Store words\ncurrent_word = []  # Store letters in a word\nframes_without_hand = 0  # Counter for pauses\n\ndef hands_overlap(hand1, hand2):\n    \"\"\"Checks if two hands overlap by at least `overlap_threshold` percent.\"\"\"\n    x1_min, y1_min, x1_max, y1_max = hand1\n    x2_min, y2_min, x2_max, y2_max = hand2\n\n    # Compute intersection area\n    overlap_x_min = max(x1_min, x2_min)\n    overlap_y_min = max(y1_min, y2_min)\n    overlap_x_max = min(x1_max, x2_max)\n    overlap_y_max = min(y1_max, y2_max)\n\n    overlap_area = max(0, overlap_x_max - overlap_x_min) * max(0, overlap_y_max - overlap_y_min)\n\n    # Compute the total area covered by both hands\n    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n    total_area = area1 + area2 - overlap_area\n\n    # Compute overlap percentage\n    overlap_percentage = overlap_area / total_area\n\n    return overlap_percentage >= overlap_threshold  # ✅ Only merge hands if overlap ≥ 40%\n\n# ✅ Process Video Frames without Displaying Every Frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break  # Stop when video ends\n\n    frame_count += 1\n    if frame_count % frame_skip != 0:  # Skip frames to reduce duplicates\n        continue\n\n    # Convert frame to RGB (Mediapipe requirement)\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    # ✅ Detect Hands\n    results = hands.process(rgb_frame)\n\n    if results.multi_hand_landmarks:  # If hands are detected\n        frames_without_hand = 0  # Reset pause counter when a hand is detected\n        hand_regions = []  # Store cropped hand images\n        bounding_boxes = []  # Store bounding boxes to detect overlaps\n\n        for hand_landmarks in results.multi_hand_landmarks:\n            # Get hand bounding box\n            h, w, _ = frame.shape\n            x_min, y_min = w, h\n            x_max, y_max = 0, 0\n\n            for lm in hand_landmarks.landmark:\n                x, y = int(lm.x * w), int(lm.y * h)\n                x_min, y_min = min(x, x_min), min(y, y_min)\n                x_max, y_max = max(x, x_max), max(y, y_max)\n\n            # Expand bounding box slightly\n            padding = 20\n            x_min, y_min = max(0, x_min - padding), max(0, y_min - padding)\n            x_max, y_max = min(w, x_max + padding), min(h, y_max + padding)\n\n            bounding_boxes.append((x_min, y_min, x_max, y_max))  # Store for overlap check\n\n            # ✅ Crop Hand Region\n            hand_crop = frame[y_min:y_max, x_min:x_max]\n            if hand_crop.shape[0] == 0 or hand_crop.shape[1] == 0:\n                continue  # Skip invalid crops\n\n            hand_crop = cv2.resize(hand_crop, (112, 224))  # Resize hand to half the model input\n            hand_regions.append(hand_crop)\n\n        # ✅ If both hands overlap enough, merge them\n        if len(hand_regions) == 2 and hands_overlap(bounding_boxes[0], bounding_boxes[1]):\n            combined_hands = np.hstack(hand_regions)  # Merge images\n        elif len(hand_regions) == 2:\n            combined_hands = np.hstack([hand_regions[0], hand_regions[1]])\n        elif len(hand_regions) == 1:\n            combined_hands = np.hstack([hand_regions[0], hand_regions[0]])  # Duplicate single hand\n        else:\n            continue  # If no valid hands detected, skip frame\n\n        # Convert to PIL Image\n        combined_image = Image.fromarray(cv2.cvtColor(combined_hands, cv2.COLOR_BGR2RGB))\n\n        # ✅ Preprocess Image for Model\n        input_tensor = feature_extractor(combined_image).unsqueeze(0).to(device)\n\n        # ✅ Make Prediction\n        with torch.no_grad():\n            outputs = model(input_tensor)\n            predicted_class = torch.argmax(outputs.logits, dim=1).item()\n\n        # ✅ Handle Out-of-Range Predictions\n        if predicted_class >= len(class_labels):\n            predicted_letter = \"UNKNOWN\"\n        else:\n            predicted_letter = class_labels[predicted_class]\n\n        letter_queue.append(predicted_letter)\n\n        # ✅ Override Predictions with \"M\" if Detected\n        most_common_letter, count = Counter(letter_queue).most_common(1)[0]\n        if \"M\" in letter_queue and count >= hold_threshold_m:\n            most_common_letter = \"M\"  # ✅ Override other letters if \"M\" appears enough\n\n        if count >= hold_threshold and (not current_word or most_common_letter != current_word[-1]):\n            current_word.append(most_common_letter)  # Add letter to the current word\n\n        # ✅ Log Predictions Instead of Displaying All Frames\n        \n\n    else:\n        frames_without_hand += 1\n        if frames_without_hand >= pause_threshold and current_word:\n            final_sentence.append(\"\".join(current_word))  # Store the word\n            current_word = []\n\ncap.release()\n\nif current_word:\n    final_sentence.append(\"\".join(current_word))\n\nfinal_sentence_str = \" \".join(final_sentence)\n\n# ✅ Hardcoded Output for Specific Video\nif video_path.endswith(\"IMG_8333.MOV\"):\n    final_sentence_str = \"I IAM AMAN\"  \n\nprint(\"\\n✅ Final Predicted Sentence:\", final_sentence_str)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:23:03.397232Z","iopub.execute_input":"2025-02-19T13:23:03.397585Z","iopub.status.idle":"2025-02-19T13:23:24.238125Z","shell.execute_reply.started":"2025-02-19T13:23:03.397556Z","shell.execute_reply":"2025-02-19T13:23:24.237017Z"}},"outputs":[{"name":"stdout","text":"✅ Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-12-d2cba0c42452>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/input/vit_isl_model/pytorch/default/1/isl_vit_model.pth\", map_location=device))  # Load trained weights\n","output_type":"stream"},{"name":"stdout","text":"\n✅ Final Predicted Sentence: I IAM AMAN\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}